{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a79b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP Block Name: fc6_0\n",
      "IP Block Name: zynq_ultra_ps_e_0\n"
     ]
    }
   ],
   "source": [
    "import pynq\n",
    "import numpy as np\n",
    "from time import time\n",
    "import struct\n",
    "overlay = pynq.Overlay(\"fc6.bit\")\n",
    "ip_dict = overlay.ip_dict\n",
    "for ip_name in ip_dict.keys():\n",
    "    print(\"IP Block Name:\", ip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0d65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LeNet\n",
      "Parsing MNIST images\n",
      "Parsing MNIST labels\n",
      "Parsing parameters\n",
      "Running inference\n",
      "\n",
      "Total Execution Time:   4547.713559 seconds (10000 images)\n",
      "Average Time per Image: 0.454771 seconds\n",
      "\n",
      "Accuracy = 98.390%\n"
     ]
    }
   ],
   "source": [
    "fc6_inst = overlay.fc6_0 \n",
    "\n",
    "# Max number of test samples in LeNet is 10,000\n",
    "NUM_TESTS = 10000\n",
    "\n",
    "# Static allocation of test images\n",
    "images = np.zeros((NUM_TESTS, 28, 28), dtype=np.uint8)\n",
    "labels = np.zeros(NUM_TESTS, dtype=np.uint8)\n",
    "\n",
    "# Static allocation of network parameters and their outputs\n",
    "image = np.zeros((1, 32, 32), dtype=np.float32)\n",
    "conv1_weights = np.zeros((6, 1, 5, 5), dtype=np.float32)\n",
    "conv1_bias = np.zeros(6, dtype=np.float32)\n",
    "conv1_output = np.zeros((6, 28, 28), dtype=np.float32)\n",
    "\n",
    "pool2_output = np.zeros((6, 14, 14), dtype=np.float32)\n",
    "\n",
    "conv3_weights = np.zeros((16, 6, 5, 5), dtype=np.float32)\n",
    "conv3_bias = np.zeros(16, dtype=np.float32)\n",
    "conv3_output = np.zeros((16, 10, 10), dtype=np.float32)\n",
    "\n",
    "pool4_output = np.zeros((16, 5, 5), dtype=np.float32)\n",
    "\n",
    "conv5_weights = np.zeros((120, 16, 5, 5), dtype=np.float32)\n",
    "conv5_bias = np.zeros(120, dtype=np.float32)\n",
    "conv5_output = np.zeros((120, 1, 1), dtype=np.float32)\n",
    "\n",
    "fc6_weights = np.zeros((10, 120, 1, 1), dtype=np.float32)\n",
    "fc6_bias = np.zeros(10, dtype=np.float32)\n",
    "fc6_output = np.zeros(10, dtype=np.float32)\n",
    "\n",
    "# Function definitions\n",
    "def relu(input):\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def convolution1(input, weights, bias, output):\n",
    "    for co in range(6):\n",
    "        for h in range(28):\n",
    "            for w in range(28):\n",
    "                conv_sum = np.sum(weights[co, 0, :, :] * input[0, h:h+5, w:w+5])\n",
    "                output[co, h, w] = conv_sum + bias[co]\n",
    "\n",
    "def relu1(input, output):\n",
    "    output[:] = relu(input)\n",
    "\n",
    "def max_pooling2(input, output):\n",
    "    for c in range(6):\n",
    "        for h in range(14):\n",
    "            for w in range(14):\n",
    "                output[c, h, w] = np.max(input[c, h*2:h*2+2, w*2:w*2+2])\n",
    "\n",
    "def relu2(input, output):\n",
    "    output[:] = relu(input)\n",
    "\n",
    "def convolution3(input, weights, bias, output):\n",
    "    for co in range(16):\n",
    "        for h in range(10):\n",
    "            for w in range(10):\n",
    "                conv_sum = np.sum(weights[co, :, :, :] * input[:, h:h+5, w:w+5])\n",
    "                output[co, h, w] = conv_sum + bias[co]\n",
    "\n",
    "def relu3(input, output):\n",
    "    output[:] = relu(input)\n",
    "\n",
    "def max_pooling4(input, output):\n",
    "    for c in range(16):\n",
    "        for h in range(5):\n",
    "            for w in range(5):\n",
    "                output[c, h, w] = np.max(input[c, h*2:h*2+2, w*2:w*2+2])\n",
    "\n",
    "def relu4(input, output):\n",
    "    output[:] = relu(input)\n",
    "\n",
    "def convolution5(input, weights, bias, output):\n",
    "    for co in range(120):\n",
    "        conv_sum = np.sum(weights[co, :, :, :] * input[:, :, :])\n",
    "        output[co, 0, 0] = conv_sum + bias[co]\n",
    "\n",
    "def relu5(input, output):\n",
    "    output[:] = relu(input)\n",
    "\n",
    "def fc6_accelerator(input, weights, bias, output):\n",
    "    input_buf = pynq.allocate(shape=input.shape, dtype=np.float32)\n",
    "    output_buf = pynq.allocate(shape=output.shape, dtype=np.float32)\n",
    "    weight_buf = pynq.allocate(shape=weights.shape, dtype=np.float32)\n",
    "    bias_buf = pynq.allocate(shape=bias.shape, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    input_buf[:] = input\n",
    "    output_buf[:] = 0.0\n",
    "    weight_buf[:] = weights\n",
    "    bias_buf[:] = bias\n",
    "    \n",
    "    \n",
    "    # Synchronize input data to the FPGA\n",
    "    input_buf.sync_to_device()\n",
    "    output_buf.sync_to_device()\n",
    "    weight_buf.sync_to_device()\n",
    "    bias_buf.sync_to_device()\n",
    "    \n",
    "\n",
    "    # Set the physical addresses of matrices input, weights, and output\n",
    "    fc6_inst.write(0x10, input_buf.physical_address)\n",
    "    fc6_inst.write(0x14, 0)\n",
    "    fc6_inst.write(0x1C, weight_buf.physical_address)\n",
    "    fc6_inst.write(0x20, 0)\n",
    "    fc6_inst.write(0x28, bias_buf.physical_address)\n",
    "    fc6_inst.write(0x2C, 0)\n",
    "    fc6_inst.write(0x34, output_buf.physical_address)\n",
    "    fc6_inst.write(0x38, 0)\n",
    "\n",
    "    fc6_inst.write(0x00, 1)  # Start accelerator\n",
    "    while fc6_inst.read(0x00) == 14:  # Wait for completion\n",
    "        pass\n",
    "\n",
    "    # Synchronize output data from the FPGA\n",
    "    output_buf.sync_from_device()\n",
    "\n",
    "    output[:] = output_buf\n",
    "\n",
    "    # Free allocated memory\n",
    "    input_buf.freebuffer()\n",
    "    output_buf.freebuffer()\n",
    "    weight_buf.freebuffer()\n",
    "    bias_buf.freebuffer()\n",
    "    \n",
    "    \n",
    "\n",
    "def parse_mnist_images(filename, images):\n",
    "    with open(filename, 'rb') as file:\n",
    "        _ = struct.unpack('>I', file.read(4))  # magic number\n",
    "        _ = struct.unpack('>I', file.read(4))  # number of images\n",
    "        _ = struct.unpack('>I', file.read(4))  # number of rows\n",
    "        _ = struct.unpack('>I', file.read(4))  # number of columns\n",
    "        images[:] = np.frombuffer(file.read(NUM_TESTS * 28 * 28), dtype=np.uint8).reshape((NUM_TESTS, 28, 28))\n",
    "\n",
    "def parse_mnist_labels(filename, labels):\n",
    "    with open(filename, 'rb') as file:\n",
    "        _ = struct.unpack('>I', file.read(4))  # magic number\n",
    "        _ = struct.unpack('>I', file.read(4))  # number of labels\n",
    "        labels[:] = np.frombuffer(file.read(NUM_TESTS), dtype=np.uint8)\n",
    "\n",
    "def parse_parameters(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        conv1_weights[:] = np.frombuffer(file.read(150 * 4), dtype=np.float32).reshape((6, 1, 5, 5))\n",
    "        conv1_bias[:] = np.frombuffer(file.read(6 * 4), dtype=np.float32)\n",
    "        conv3_weights[:] = np.frombuffer(file.read(2400 * 4), dtype=np.float32).reshape((16, 6, 5, 5))\n",
    "        conv3_bias[:] = np.frombuffer(file.read(16 * 4), dtype=np.float32)\n",
    "        conv5_weights[:] = np.frombuffer(file.read(48000 * 4), dtype=np.float32).reshape((120, 16, 5, 5))\n",
    "        conv5_bias[:] = np.frombuffer(file.read(120 * 4), dtype=np.float32)\n",
    "        fc6_weights[:] = np.frombuffer(file.read(1200 * 4), dtype=np.float32).reshape((10, 120, 1, 1))\n",
    "        fc6_bias[:] = np.frombuffer(file.read(10 * 4), dtype=np.float32)\n",
    "\n",
    "def get_image(images, idx, image):\n",
    "    image[:] = -1.0\n",
    "    image[0, 2:30, 2:30] = images[idx] / 255.0 * 2.0 - 1.0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting LeNet\")\n",
    "\n",
    "    print(\"Parsing MNIST images\")\n",
    "    parse_mnist_images(\"images.bin\", images)\n",
    "\n",
    "    print(\"Parsing MNIST labels\")\n",
    "    parse_mnist_labels(\"labels.bin\", labels)\n",
    "\n",
    "    print(\"Parsing parameters\")\n",
    "    parse_parameters(\"params.bin\")\n",
    "\n",
    "    print(\"Running inference\")\n",
    "    num_correct = 0\n",
    "\n",
    "    # starting time\n",
    "    t1 = time()\n",
    "\n",
    "    for k in range(NUM_TESTS):\n",
    "        # Get test image from dataset\n",
    "        get_image(images, k, image)\n",
    "\n",
    "        # Begin inference here.\n",
    "        convolution1(image, conv1_weights, conv1_bias, conv1_output)\n",
    "        relu1(conv1_output, conv1_output)\n",
    "\n",
    "        max_pooling2(conv1_output, pool2_output)\n",
    "        relu2(pool2_output, pool2_output)\n",
    "\n",
    "        convolution3(pool2_output, conv3_weights, conv3_bias, conv3_output)\n",
    "        relu3(conv3_output, conv3_output)\n",
    "\n",
    "        max_pooling4(conv3_output, pool4_output)\n",
    "        relu4(pool4_output, pool4_output)\n",
    "\n",
    "        convolution5(pool4_output, conv5_weights, conv5_bias, conv5_output)\n",
    "        relu5(conv5_output, conv5_output)\n",
    "\n",
    "        # Use the FPGA-based fc6 accelerator\n",
    "        fc6_accelerator(conv5_output, fc6_weights, fc6_bias, fc6_output)\n",
    "        # Inference ends here.\n",
    "\n",
    "        # Index of the largest output is result\n",
    "        # Check which output was the largest.\n",
    "        result = np.argmax(fc6_output)\n",
    "\n",
    "        if result == labels[k]:\n",
    "            num_correct += 1\n",
    "\n",
    "    # ending time\n",
    "    t2 = time()\n",
    "    time_span = t2 - t1\n",
    "    print(\"\\nTotal Execution Time:   {:.6f} seconds ({:d} images)\".format(time_span, NUM_TESTS))\n",
    "    print(\"Average Time per Image: {:.6f} seconds\".format(time_span / NUM_TESTS))\n",
    "    print(\"\\nAccuracy = {:.3f}%\".format(num_correct / NUM_TESTS * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47f8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
